# -*- coding: utf-8 -*-
"""ML System That Classifies Which Tweets Are Toxic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qKkwT-8LQ20AFXtFvGxfy_hC9hOcDxVv
"""

import pandas as pd
import numpy as np

from google.colab import files
uploaded = files.upload()

import pandas as pd
data_df = pd.read_csv("tweets_flagged_v2.csv")
data_df = data_df.drop(columns = ["Unnamed: 0"])

data_df["Toxicity"].value_counts()

data_df.head(5)

data_df.shape

data_df["Toxicity"].value_counts()

for _ in range(10):
    random_ind = np.random.randint(0, len(data_df))
    random_data = data_df.iloc[random_ind]
    print(random_data["tweet"], random_data["Toxicity"])

from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

X = data_df["tweet"].values
y = data_df["Toxicity"].values

sequences = [sequence for sequence in X]
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

model_inputs

import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((model_inputs['input_ids'],y))

dataset = dataset.cache()
dataset = dataset.shuffle(160000)
dataset = dataset.batch(16)
dataset = dataset.prefetch(8)

# training pipeline
train = dataset.take(int(len(dataset)*0.7))
val = dataset.skip(int(len(dataset)*0.7)).take(int(len(dataset)*0.2))
test = dataset.skip(int(len(dataset)*0.9)).take(int(len(dataset)*0.1))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense

model = Sequential(name="text-classifier")
model.add(Embedding(len(tokenizer.get_vocab()), 32))
model.add(Bidirectional(LSTM(32, activation='tanh')))

model.add(Dense(128, activation = 'relu'))
model.add(Dense(256, activation = 'relu'))
model.add(Dense(128, activation = 'relu'))

model.add(Dense(1, activation = 'sigmoid'))

model.summary()

model.compile(loss = "binary_crossentropy", optimizer = "Adam")

with tf.device("/device:GPU:0"):  # ensures GPU usage if available
    history = model.fit(
        train,                   # your training data
        epochs=1,                # number of passes through dataset
        batch_size=16,           # how many samples per gradient update
        validation_data=val      # validation data tuple
    )

from tensorflow.keras.metrics import Precision, Recall, Accuracy
import numpy as np


pre = Precision()
rec = Recall()
acc = Accuracy()


for batch in test.as_numpy_iterator():
    x_true, y_true = batch

    y_hat = model.predict(x_true)
    y_hat = y_hat.squeeze()

    pre.update_state(y_true, y_hat)
    rec.update_state(y_true, y_hat)
    acc.update_state(y_true, y_hat)

# Print final results
print("Precision:", pre.result().numpy())
print("Recall:", rec.result().numpy())
print("Accuracy:", acc.result().numpy())

import numpy as np


y_true = np.concatenate([y for x, y in test.as_numpy_iterator()])
y_hat = model.predict(test)


prob_accuracy = 1 - np.mean(np.abs(y_true - y_hat))

print("Probability-based accuracy:", prob_accuracy)